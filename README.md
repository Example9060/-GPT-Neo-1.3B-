# Дообучение GPT-Neo 1.3B на русскоязычном новостном корпусе


##  Используемые инструменты

- **Модель**: [`EleutherAI/gpt-neo-1.3B`](https://huggingface.co/EleutherAI/gpt-neo-1.3B)
- **Архитектура**: Causal LM + LoRA (Low-Rank Adaptation)
- **Фреймворки**: HuggingFace Transformers, PEFT, Datasets, Accelerate, Bitsandbytes
- **Аппаратные ресурсы**: 1×GPU 14GB VRAM (дообучение с помощью 8bit и LoRA)

---

##  Данные

Использован открытый корпус новостей:
-  **Источник**: [Kaggle Lenta.ru News Corpus](https://www.kaggle.com/datasets/yutkin/corpus-of-russian-news-articles-from-lenta)
-  10000 текстов
-  Обработка: объединение `title` и `text`, ограничение длины до 512 токенов, токенизация с паддингом и обрезкой.

---

##  Обучение

- Модель загружена в 8-bit режиме (`load_in_8bit=True`)
- Применён LoRA-тюнинг (`r=8`, `alpha=16`, `dropout=0.1`)
- Использован `Trainer` с паддингом, `DataCollatorForLanguageModeling`
- Обучение на 1 эпохах при `batch_size=1`, `gradient_accumulation=8`
- В связи с лимитами вычислительных мощностей обучение проходило в  **одной** эпохе, но это и не помешало показать хорошие результаты 

---

##  Оценка

###  Перплексия:
- `Perplexity ≈ 2.46` — хороший показатель понимания текста моделью

###  BLEU / ROUGE:
- `BLEU ≈ 0.000003`, `ROUGE-L ≈ 0.0`
Мы здесь видим низике показтели для BLEU и ROUGE-L. Но если вспомнить что они отвечают или скорее показываеют, то насколько предложенний ответ схож с референсом.То это объяснеяет их низкий показатель. И тем самым показываает что данные метрики не совсем уместны для данной задачи. На  примерах мы можем видеть, что модель отрабатывает хорошо и дает осмысленный ответ по промпту
Примеры прикрепил ниже:
(см. `example.png`)

---






